# encoding: utf-8
""" Generate sequences by sampling tokens. """
import itertools as it
from typing import Generator, List, Union
import warnings

import torch
import torch.nn.functional as F

from .writer import Writer
from txt.models import Model


__all__ = ["Sampler"]


class Sampler(Writer):
    """An iterator that generates tokens using the sampling method.

    Sampling is an auto-regressive method that consists in choosing at each
    step a token randomly according to their probability, given the previous
    tokens. Several transformations can be applied to the tokens' probability
    distribution: the temperature can be changed, we can penalize repeated
    tokens or apply k-filtering or nucleus filtering.

    The default values are chosen so that k-filtering, nucleus filtering,
    temperature and repetition penalty are disabled.

    Attributes
    ----------
    model: `object`
        Any object that implements the `decode` method that takes a list
        of token ids and returns a probability distribution over the
        vocabulary.
    device: `torch.device`
        Device on which the computations will be run.
    k: int between 0 and vocab_size
        Parameter for the top-k filtering
    p: float between 0 and 1
        Parameter for the nucleus filtering
    temperature: strictly positive float
        Parameter used to modulate the distribution over ids. Low temperatures
        put more emphasis on highly probably token while high temperatures tend
        to smooth the probability distribution.
    repetition_penalty: strictly postitive float
        The penalty applied to repeating token ids.

    Yields
    ------
    int
        Token id generated by the sampler.

    Raises
    ------
    ValueError
        When the values of p and k are out of bounds.
    ZeroDivisionError
        When temperature and repetition_penalty are set to 0.

    Warnings
    --------
    When the temperature is set to a negative value.
    When the repetition penalty is set to a negative value.
    When the value of k is larger than the vocabulary size.
    """

    def __init__(
        self,
        model: Model,
        device: torch.device,
        k: int = 0,
        p: float = 0.0,
        temperature: float = 1.0,
        repetition_penalty: float = 1.0,
    ) -> None:

        super(Sampler, self).__init__(model)
        self.k = k
        self.p = p
        self.temperature = temperature
        self.repetition_penalty = repetition_penalty

        self.validate_input_parameters()

    def validate_input_parameters(self) -> None:
        """Validate the values of the input parameters.

        We validate in the class initialization to avoid the overhead of
        checking at every iteration.
        """

        # k filtering
        if self.k < 0:
            raise ValueError(
                """ You are trying to apply k-filtering with a value of k that is smaller
                or equal to 0 ({}). k represents a number of tokens, it must be strictly
                positive. If you intended to disable k-filtering, set its value to 0.""".format(
                    self.k
                )
            )

        # nucleus filtering
        if self.p > 1 or self.p < 0:
            raise ValueError(
                """You are trying to apply nucleus filtering with a value of p greater than 1  or smaller than 0 ({}).
                However p is a probability and its value must lie between 0 and 1. If you want to
                disable nucleus filtering set a value of 0 instead.""".format(
                    self.p
                )
            )

        # temperature
        if self.temperature == 0:
            raise ZeroDivisionError(
                """You are trying to sample with a temperature equal to 0.
                If you did not want to apply temperature, set its value to 1 or omit
                the argument instead.
                Otherwise set the temperature to a value different from 0."""
            )
        elif self.temperature < 0:
            warnings.warn(
                """You are trying to apply a temperature that is strictly negative ({}).
                Are you sure this is what you want to do?""".format(
                    self.temperature
                )
            )

        # repetition penalty
        if self.repetition_penalty == 0:
            raise ZeroDivisionError(
                """You are trying to apply repetition_penalty with a penalty equal to 0.
                If you did not want to apply repetition penalty set its value to 1. instead,
                or omit the argument.
                Otherwise set the repetition penalty to a value different from 0."""
            )
        elif self.repetition_penalty < 0:
            warnings.warn(
                """You are trying to apply a repetition penalty that is strictly negative ({}).
                Are you sure this is what you want to do?""".format(
                    self.temperature
                )
            )

    def tokens(self) -> Generator[int, None, None]:
        past = self.past
        while True:
            next_token_logits = self.model.decode(self.past)
            logits = self.apply_repetition_penalty(next_token_logits, past)
            logits = self.apply_temperature(logits)
            logits = self.apply_top_k_filter(logits)
            logits = self.apply_nucleus_filter(logits)
            next_token = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1)
            past = torch.cat((past, next_token), dim=1)
            yield next_token.squeeze(-1).squeeze(-1).item()

    def apply_repetition_penalty(
        self, logits: torch.tensor, past_sequence: torch.tensor
    ) -> torch.tensor:
        """ Apply a penalty to tokens that appear more than once in the
        generated sequence.

        .. Keskar, Nitish Shirish, et al. "Ctrl: A conditional transformer
           language model for controllable generation." arXiv preprint
           arXiv:1909.05858 (2019).
        """
        generated_token_idx = set(past_sequence[0].tolist())
        for token_idx in generated_token_idx:
            logits[0, token_idx] /= self.repetition_penalty
        return logits

    def apply_temperature(self, logits: torch.tensor) -> torch.tensor:
        """ Shape the tokens' distribution through temperature. The higher the value
        of the temperature, the more skewed towards high probability events the
        distribution is.

        .. Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. Deep learning.
           MIT press, 2016.
        """
        return logits / self.temperature

    def apply_top_k_filter(self, logits: torch.tensor) -> torch.tensor:
        """ Use the probability distribution of the tokens to determine the set
        to be sampled from. Specifically we select the set of size k such that
        the sum of its items' probabilities is maximum.

        .. Fan, Angela, Mike Lewis, and Yann Dauphin. "Hierarchical neural
           story generation." arXiv preprint arXiv:1805.04833 (2018).
        """
        if self.k == 0:
            return logits

        vocabulary_size = logits.size(-1)
        if self.k > vocabulary_size:
            warnings.warn(
                """You provided a value for k ({}) that is larger than the vocabulary size ({}).
                We adjusted k's value to the vocabulary size; if that was what you intended to do
                we recommend setting k to 0 instead. It this is not the behavior you expected,
                choose a value of k that is smaller than the vocabulary size.""".format(
                    self.k, vocabulary_size
                )
            )
            self.k = vocabulary_size

        indices_to_remove = logits < torch.topk(logits, self.k)[0][..., -1, None]
        logits[indices_to_remove] = -float("Inf")

        return logits

    def apply_nucleus_filter(self, logits: torch.tensor) -> torch.tensor:
        """ Use the probability distribution of the tokens to determine the set
        to be sampled from. Specifically, choose the smallest set such that the
        sum of its items' probabilities is greater than a number p in [0,1].

        .. Holtzman, Ari, et al. "The curious case of neural text
           degeneration." arXiv preprint arXiv:1904.09751 (2019).
        """
        if self.p == 0:
            return logits

        sorted_logits, sorted_indices = torch.sort(logits, descending=True)
        sorted_probabilities = F.softmax(sorted_logits, dim=-1)
        cumulative_probabilities = torch.cumsum(sorted_probabilities, dim=-1)

        # Remove tokens with cumulative probability above the threshold,
        # but keep the first token above the threshold.
        sorted_indices_to_remove = cumulative_probabilities > self.p
        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
        sorted_indices_to_remove[..., 0] = 0

        # scatter sorted tensors to original indexing
        indices_to_remove = sorted_indices_to_remove.scatter(
            dim=-1, index=sorted_indices, src=sorted_indices_to_remove
        )
        logits[indices_to_remove] = -float("Inf")

        return logits

    def generate_ids(self, num_tokens: int) -> List[int]:
        """Generate a sequence of fixed length.

        Parameter
        ---------
        num_tokens: int
            The number of tokens to generate (in addition to the prompt
            if one is provided).

        Returns
        -------
        List[int]
            A list with the generated tokens.
        """
        tokens = self.tokens()
        return list(it.islice(tokens, num_tokens))

    def generate_ids_until(
        self,
        end_tokens: Union[int, List[int]],
        max_length: int = 100,
        min_length: int = 1,
    ) -> List[int]:
        """Generate a sequence until a set token is generated.

        Attributes
        ----------
        end_tokens: int or List[int]
            The token or sequence of tokens that stops the process when generated.
        max_length: int
            The maximum length of the generated sequence. The process will stop
            at `max_length` even if the `end_token` has not been generated. Prevents
            infinite loops.
        min_length: int
            The minimum length of the generated sequence. If `end_token` is
            generated and the sequence is shorter than `min_length` the generation
            continues.

        Returns
        -------
        List[int]
            A list with the generated tokens that includes the `end_tokens`
            if the generation stopped before `max_length`.
        """
        sequence = []

        if isinstance(end_tokens, int):
            end_tokens = [end_tokens]
        last_token = end_tokens[-1]

        tokens = self.tokens()
        for token in tokens:
            sequence.append(token)
            if token == last_token:
                if len(sequence) < min_length:
                    continue
                if sequence[-len(end_tokens) :] == end_tokens:
                    break
            if len(sequence) == max_length:
                break

        return sequence
